import torch
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
import torch.nn.functional as F

# --- åŠ è½½ä½ çš„æ¨¡å‹ ---
MODEL_PATH = "/home/fulian/RAG/Qwen_embed"
device = "cuda" if torch.cuda.is_available() else "cpu"
embed_model = HuggingFaceEmbedding(model_name=MODEL_PATH, device=device, trust_remote_code=True)

def compare_sentences(sent_garbage, sent_useful):
    # 1. è®¡ç®—ä¸¤ä¸ªå¥å­çš„å‘é‡
    emb1 = torch.tensor(embed_model.get_text_embedding(sent_garbage)).to(device)
    emb2 = torch.tensor(embed_model.get_text_embedding(sent_useful)).to(device)
    
    # 2. è®¡ç®—ç›¸ä¼¼åº¦ (Cosine Similarity)
    similarity = F.cosine_similarity(emb1.unsqueeze(0), emb2.unsqueeze(0)).item()
    
    print(f"ğŸ—‘ï¸ åƒåœ¾å¥: {sent_garbage}")
    print(f"ğŸ’ æœ‰ç”¨å¥: {sent_useful}")
    print(f"ğŸ“Š ç›¸ä¼¼åº¦: {similarity:.4f}")
    
    # æ¨¡æ‹Ÿé˜ˆå€¼æµ‹è¯•
    threshold = 0.85  # å‡è®¾æˆ‘ä»¬è®¾å®šçš„åˆ é™¤çº¿
    if similarity > threshold:
        print("âš ï¸ å±é™©ï¼è¿™ä¸¤ä¸ªå¥å­å¤ªåƒäº†ï¼Œå¯èƒ½ä¼šè¢«è¯¯åˆ ï¼")
    else:
        print("âœ… å®‰å…¨ã€‚æœ‰ç”¨å¥å­çš„å·®å¼‚è¶³ä»¥è®©å®ƒé€ƒè¿‡è¿‡æ»¤å™¨ã€‚")
    print("-" * 30)

# --- æµ‹è¯•æ¡ˆä¾‹ ---
# Case 1: æåº¦ç›¸ä¼¼
compare_sentences("Can you hear the frequency?", "Now do you understand the frequency?")

# Case 2: ç»“æ„ç›¸ä¼¼ä½†å†…å®¹ä¸åŒ
compare_sentences("Any questions?", "Any questions about the exam?")

# Case 3: ä½ çš„æ‹…å¿ƒ
compare_sentences("Let's get started.", "Let's get started with Python.")